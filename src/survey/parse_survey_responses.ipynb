{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b28f871d-b1bc-4127-ba22-a4924e927d06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"/Volumes/sql_workshop/sandbox/responses\"\n",
    "df = spark.read.option(\"delimiter\", \"\\t\").option(\"header\", \"true\").csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1436890-72a1-43db-b5da-4a3368a19ffb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dropping unwanted columns by index\n",
    "columns_to_drop = [df.columns[1], df.columns[14], df.columns[15]]\n",
    "df = df.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35d0c919-3c1f-4fb4-a4fb-8866492b93af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (df.withColumnRenamed(\"Timestamp\", \"timestamp\")\n",
    "       .withColumnRenamed(\"What most closely describes your current role?\", \"role\")\n",
    "       .withColumnRenamed(\"Please rate your experience level with the Databricks platform\", \"experience\")\n",
    "       .withColumnRenamed(\"What data warehousing activities apply most to your day-to-day job?\", \"activities\")\n",
    "       .withColumnRenamed(\"What do you hope to get out of this workshop?\", \"hope_to_get\")\n",
    "       .withColumnRenamed(\"Are there specific topics that you are interested in? If yes, please indicate below.\", \"topics\")\n",
    "       .withColumnRenamed(\"Do you have any other comments, feedback, or anything else you think we should know?\", \"comments\")\n",
    "       .withColumnRenamed(\"Please rate your technical knowledge of the following languages/technologies: [SQL]\", \"sql\")\n",
    "       .withColumnRenamed(\"Please rate your technical knowledge of the following languages/technologies: [Python]\", \"python\")\n",
    "       .withColumnRenamed(\"Please rate your technical knowledge of the following languages/technologies: [R]\", \"r\")\n",
    "       .withColumnRenamed(\"Please rate your technical knowledge of the following languages/technologies: [Spark]\", \"spark\")\n",
    "       .withColumnRenamed(\"Please rate your technical knowledge of the following languages/technologies: [Scala]12\", \"scala\")\n",
    "       .withColumnRenamed(\"Would you be remote or in-person?\", \"remote\"))\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de92ca39-1dfd-4d70-a75b-af87926ce8d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save df as delta table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sql_workshop.sandbox.responses_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1173aed8-2f23-4ddb-bd59-cda4e0c5d152",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode, col, trim, when\n",
    "\n",
    "# Split the \"role\" column by \",\" and explode into their own row, removing leading/trailing spaces\n",
    "df_roles = df.select(explode(split(trim(col(\"role\")), \",\")).alias(\"role\")).withColumn(\"role\", trim(col(\"role\")))\n",
    "\n",
    "# Define the categories\n",
    "categories = [\"Data Analyst\", \"Data Engineer\", \"Hybrid Data Analyst and Data Engineer\", \"Data scientist\"]\n",
    "\n",
    "# Categorize roles and group by role with count aggregation\n",
    "df_roles = df_roles.withColumn(\"role\", when(col(\"role\").isin(categories), col(\"role\")).otherwise(\"other\"))\n",
    "df_roles = df_roles.groupBy(\"role\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "display(df_roles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a39cfb68-cbec-4c18-b021-8f59964a7c1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode, col, trim, when\n",
    "\n",
    "# Split the \"activities\" column by \",\" and explode into their own row, removing leading/trailing spaces\n",
    "df_activities = df.select(explode(split(trim(col(\"activities\")), \",\")).alias(\"activities\")).withColumn(\"activities\", trim(col(\"activities\")))\n",
    "\n",
    "# Define the categories\n",
    "# categories = [\"Data Analyst\", \"Data Engineer\", \"Hybrid Data Analyst and Data Engineer\", \"Data scientist\"]\n",
    "\n",
    "# Categorize roles and group by role with count aggregation\n",
    "# df_roles = df_roles.withColumn(\"role\", when(col(\"role\").isin(categories), col(\"role\")).otherwise(\"other\"))\n",
    "df_activities = df_activities.groupBy(\"activities\").count().orderBy(col(\"activities\").desc())\n",
    "\n",
    "display(df_activities)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "parse_survey_responses",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
